'''
Generate ONE trajectory with greedy search


Greedy search is a simple optimization method that iteratively selects the best action at each step.
`n_action_try` is the number of actions to try at each step.

'''

import os
import sys

path = os.path.dirname(os.path.abspath(__file__))
# Add project root to Python path for multi-branch development
project_root = os.path.abspath(os.path.join(path, '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import numpy as np
import multiprocessing as mp
import time
from typing import Dict, Any

from pyTSFoil.environment.basic import MultiBumpModificationAction
from pyTSFoil.environment.utils import TSFoilEnv_FigState_MultiBumpAction
from model.database import AirfoilDatabase


N_ACTION_TRY = 10
N_MAX_STEP = 5
N_ENVS = 10000
N_PROCESSES = 100


def create_env_with_id(initial_airfoil='random-selected', n_max_step=N_MAX_STEP,
                        render_mode='none'):
    '''
    Factory function to create a new environment instance with unique worker ID
    
    This function is called by each worker process to create its own
    isolated environment instance. Each worker gets completely separate
    PyTSFoil/Fortran data to avoid conflicts.
    
    Parameters:
    ----------
    initial_airfoil: int|str|np.ndarray
        The initial airfoil.
        - 'random-interpolated': random interpolated airfoil from the database.
        - 'random-selected': random select original airfoil from the database.  (default)
        - int: the airfoil ID from the database. RAE2822 is 10.
        - np.ndarray: the airfoil coordinates.
    '''
    # Create sample airfoil
    database = AirfoilDatabase(fname_database=os.path.join(path, 'selected-airfoils-cst.dat'))
    
    # Custom action class
    action_class = MultiBumpModificationAction()
    
    # Define custom digit precision for the action, so that it's easier for LLM to learn
    action_class.action_precision = []
    for i in range(action_class.dim_action):
        if i % 2 == 0:
            action_class.action_precision.append(2)
        else:
            action_class.action_precision.append(3)

    # Create unique output directory for each worker to avoid file conflicts
    if isinstance(initial_airfoil, int):
        worker_output_dir = None
    else:
        worker_output_dir = path

    # Create environment instance
    return TSFoilEnv_FigState_MultiBumpAction(
        database=database,
        initial_airfoil=initial_airfoil,
        output_dir=worker_output_dir,
        render_mode=render_mode,
        path_save_fig_of_observation=None,
        action_class=action_class,
        n_max_step=n_max_step
    )


def greedy_search_worker(params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Each worker runs independently and generates a greedy search trajectory.
    
    Parameters:
    -----------
    params: Dict[str, Any]
        Dictionary containing:
        - 'worker_id': Unique identifier for this worker
        - 'n_max_step': Number of steps this worker should generate
        - 'n_action_try': Number of actions to try at each step
        - 'fname_prefix': Prefix for the filename of the trajectory


    Returns:
    --------
    Dict[str, Any]: Greedy search data generated by this worker
    """
    try:

        # Extract parameters
        worker_id = params['worker_id'] 
        n_max_step = params['n_max_step']
        n_action_try = params['n_action_try']
        fname_prefix = params['fname_prefix']
        render_mode = params['render_mode']
        
        # CRITICAL: Seed random number generator differently for each process
        # This ensures each worker generates different random sequences
        seed = hash((os.getpid(), worker_id, time.time())) % (2**32)
        np.random.seed(seed)
        # np.random.seed(12345 + worker_id)  # Reproducible across runs

        # Create environment in this process
        env = create_env_with_id(initial_airfoil=worker_id, n_max_step=n_max_step, render_mode=render_mode)
        
        if render_mode == 'save':
            env.path_save_fig_trajectory = os.path.join(path, 'figures')
            env.render_fig_fname = f'{fname_prefix}-{worker_id}.png'
            env.render_dpi = 50
        
        # Reset environment and get initial state
        env.reset()
        
        rewards = []

        for step in range(n_max_step):

            _, reward, done, info = env.step_with_greedy_search(n_action_try=n_action_try)
            
            rewards.append(reward)

            if done:
                break
            
        env.render()
        env.save_trajectory(fname_json=os.path.join(path, 'trajectories', f'{fname_prefix}-{worker_id}.json'))
                
        return {
            'worker_id': worker_id,
            'success': True,
            'rewards': np.array(rewards),
            'total_reward': env.total_reward,
        }
        
    except Exception as e:
        print(f"Worker {worker_id} error: {str(e)}")
        return {
            'worker_id': worker_id,
            'success': False,
            'error': str(e)
        }


class ParallelGreedySearch(object):
    '''
    Parallel greedy search
    
    Parameters:
    -----------
    n_action_try: int
        The number of actions to try at each step.
    n_max_step: int
        The maximum number of steps to take.
    n_processes: int
        The number of processes to use.
    n_envs: int
        The number of environments to use.
    fname_prefix: str
        The prefix for the filename of the trajectory.

    '''
    def __init__(self, n_action_try: int, n_max_step: int, n_processes: int, n_envs: int, fname_prefix: str, render_mode: str):
        
        self.n_action_try = n_action_try
        self.n_max_step = n_max_step
        self.n_processes = n_processes
        self.n_envs = n_envs
        self.fname_prefix = fname_prefix
        self.render_mode = render_mode
        
    def generate_trajectories(self):
        '''
        Generate greedy search trajectories in parallel
        '''
        # Prepare parameters for each worker
        worker_params = []
        for worker_id in range(self.n_envs):
            params = {
                'worker_id': worker_id,
                'n_max_step': self.n_max_step,
                'n_action_try': self.n_action_try,
                'fname_prefix': self.fname_prefix,
                'render_mode': self.render_mode,
            }
            worker_params.append(params)
        
        # Run rollout collection in parallel using Pool
        start_time = time.time()
        
        with mp.Pool(processes=self.n_processes) as pool:
            worker_results = pool.map(greedy_search_worker, worker_params)
        
        elapsed_time = time.time() - start_time

        print(f"Greedy search trajectories generated in {elapsed_time:.2f} seconds")

        # Process results from workers
        successful_results = [r for r in worker_results if r['success']]
        failed_results = [r for r in worker_results if not r['success']]
        
        if failed_results:
            print(f"Warning: {len(failed_results)} workers failed:")
            for result in failed_results:
                print(f"  Worker {result['worker_id']}: {result['error']}")
        
        if not successful_results:
            raise RuntimeError("All workers failed to generate trajectories")



if __name__ == '__main__':
    
    print('Generate ONE trajectory with greedy search')
    print('path: ', path)


    os.makedirs(os.path.join(path, 'trajectories'), exist_ok=True)
    os.makedirs(os.path.join(path, 'figures'), exist_ok=True)

    if N_ENVS <= 1:

        params = {
            'worker_id': 10,
            'n_action_try': N_ACTION_TRY,
            'n_max_step': N_MAX_STEP,
            'fname_prefix': 'trajectory',
            'render_mode': 'save',
        }
        
        result = greedy_search_worker(params)
        
        print('total reward: ', result['total_reward'])

    else:
        
        parallel_greedy_search = ParallelGreedySearch(
            n_action_try=N_ACTION_TRY,
            n_max_step=N_MAX_STEP,
            n_processes=N_PROCESSES,
            n_envs=N_ENVS,
            fname_prefix='trajectory',
            render_mode='save',
        )

        parallel_greedy_search.generate_trajectories()
